# BoneXage-assessment
Predicting children's bone age using hand X-ray images
# ğŸ¦´ Bone Age Assessment using ConvNeXt & GeM Pooling

[![Python](https://img.shields.io/badge/Python-3.8%2B-blue)](https://www.python.org/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0%2B-orange)](https://pytorch.org/)
[![License](https://img.shields.io/badge/license-MIT-green)](LICENSE)

A PyTorch implementation for precise pediatric **Bone Age Assessment (BAA)** based on hand radiographs.

This project utilizes a modern CNN architecture (**ConvNeXt**) combined with **Generalized Mean (GeM) Pooling** and specific optimizations for medical regression tasks. It achieves superior convergence and stability on the RSNA Pediatric Bone Age dataset compared to traditional baselines.

## ğŸš€ Key Features

* **SOTA Backbone**: Utilizes `ConvNeXt-Tiny` as the feature extractor, offering stronger feature representation than ResNet50 or EfficientNet.
* **GeM Pooling**: Implements learnable Generalized Mean Pooling instead of standard Average Pooling to better capture high-response features (e.g., ossification centers).
* **Multi-Modal Fusion**: Integrates image features with gender encoding to improve prediction accuracy significantly.
* **Numerical Stability**: Applies `Sigmoid * Max_Age` scaling at the output layer. This constraint prevents negative predictions and gradient explosion, ensuring stable training dynamics.
* **AMP Support**: Implements Automatic Mixed Precision (AMP) for faster training and lower GPU memory usage.

## ğŸ› ï¸ Requirements

Ensure you have Python installed. You can install the dependencies via pip:

```bash
pip install torch torchvision numpy pandas matplotlib scikit-learn tqdm pillow

##ğŸ“‚ Dataset Preparation
This project uses the RSNA Pediatric Bone Age Challenge dataset. Please organize your data directory as follows:
data/
â”œâ”€â”€ boneage-training-dataset/   # Training images
â”œâ”€â”€ boneage-test-dataset/       # Test images
â”œâ”€â”€ boneage-training-dataset.csv
â””â”€â”€ boneage-test-dataset.csv


## ğŸ–¥ï¸ Usage
1. Training

Run train.py to start training. The script automatically detects available GPUs and utilizes DataParallel if applicable.
python train.py

2. Inference

Use predict.py to predict the bone age for a single image.
python predict.py <image_list_filename> <sex(M/F)> <image_size> <model_path>

Output:
data/test_imgs/1234.png: 10 years 6 months (126.5 months)

## ğŸ§  Model Architecture
graph LR
    A[Input Image 512x512] --> B[ConvNeXt Backbone]
    B --> C[Feature Maps]
    C --> D[GeM Pooling]
    D --> E[Image Features]
    
    F[Input Sex] --> G[Sex Encoder]
    G --> H[Sex Features]
    
    E --> I[Concat]
    H --> I
    I --> J[FC Layers + Mish]
    J --> K[Sigmoid * 240]
    K --> L[Predicted Age (Months)]

## ğŸ“Š Results
Metric,Value (Approx)
MAE (Mean Absolute Error),To be updated
Backbone,ConvNeXt-Tiny
Input Size,512x512

##ğŸ“ Acknowledgments
    Dataset provided by the RSNA Pediatric Bone Age Challenge.
    ConvNeXt architecture based on A ConvNet for the 2020s.


%% å°†æ­¤ä»£ç å—å¤åˆ¶åˆ°ä½ çš„ GitHub README.md ä¸­
graph TD
    %% å®šä¹‰æ ·å¼
    classDef input fill:#E1F5FE,stroke:#0288D1,stroke-width:2px,color:#01579B;
    classDef output fill:#E8F5E9,stroke:#388E3C,stroke-width:2px,color:#1B5E20;
    classDef backbone fill:#FFF3E0,stroke:#FF9800,stroke-width:2px,color:#E65100;
    classDef pooling fill:#F3E5F5,stroke:#9C27B0,stroke-width:2px,color:#4A148C;
    classDef fusion fill:#FCE4EC,stroke:#C2185B,stroke-width:2px,color:#880E4F;
    classDef head fill:#FFFDE7,stroke:#FBC02D,stroke-width:2px,color:#F57F17;
    classDef tensor fill:#FFFFFF,stroke:#9E9E9E,stroke-width:1px,stroke-dasharray: 5 5;

    %% è¾“å…¥å±‚
    subgraph Inputs [è¾“å…¥æ•°æ®]
        IMG[å›¾åƒè¾“å…¥<br>Batch x 3 x 512 x 512]:::input
        SEX[æ€§åˆ«è¾“å…¥<br>Batch x 1 (0æˆ–1)]:::input
    end

    %% å›¾åƒå¤„ç†åˆ†æ”¯
    subgraph Image Branch [å›¾åƒç‰¹å¾æå–åˆ†æ”¯]
        BACKBONE[ConvNeXt-Tiny Backbone<br>(é¢„è®­ç»ƒæƒé‡)]:::backbone
        FEAT_MAPS(ç‰¹å¾å›¾<br>Batch x 768 x H' x W'):::tensor
        GEM[GeM Pooling<br>(å¹¿ä¹‰å¹³å‡æ± åŒ–)]:::pooling
        IMG_VEC(å›¾åƒç‰¹å¾å‘é‡<br>Batch x 768):::tensor
    end

    %% æ€§åˆ«å¤„ç†åˆ†æ”¯
    subgraph Sex Branch [æ€§åˆ«ç¼–ç åˆ†æ”¯]
        SEX_ENC[Sex Encoder MLP<br>Linear-BN-ReLU x2]:::block
        SEX_VEC(æ€§åˆ«ç‰¹å¾å‘é‡<br>Batch x 32):::tensor
    end

    %% ç‰¹å¾èåˆ
    CONCAT{ç‰¹å¾æ‹¼æ¥<br>Concatenation}:::fusion
    FUSED_VEC(èåˆç‰¹å¾å‘é‡<br>Batch x 800):::tensor

    %% å›å½’å¤´
    subgraph Regression Head [å›å½’é¢„æµ‹å¤´]
        L1[BN + Dropout(0.5)]:::head
        L2[Linear(800->512) + Mishæ¿€æ´»]:::head
        L3[BN + Dropout(0.4)]:::head
        L4[Linear(512->64) + Mishæ¿€æ´»]:::head
        L5[Linear(64->1)]:::head
    end

    %% è¾“å‡ºç¼©æ”¾
    SCALING[Range Scaling<br>Sigmoid * MaxAge(240)]:::fusion
    FINAL_OUT(æœ€ç»ˆé¢„æµ‹éª¨é¾„<br>Batch x 1 (æœˆ)]:::output

    %% è¿æ¥å…³ç³»
    IMG --> BACKBONE
    BACKBONE --> FEAT_MAPS
    FEAT_MAPS --> GEM
    GEM --> IMG_VEC

    SEX --> SEX_ENC
    SEX_ENC --> SEX_VEC

    IMG_VEC --> CONCAT
    SEX_VEC --> CONCAT
    CONCAT --> FUSED_VEC

    FUSED_VEC --> L1
    L1 --> L2
    L2 --> L3
    L3 --> L4
    L4 --> L5

    L5 --> SCALING
    SCALING --> FINAL_OUT
