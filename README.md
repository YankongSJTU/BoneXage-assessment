graph TD
    %% å®šä¹‰æ ·å¼
    classDef input fill:#E3F2FD,stroke:#1565C0,stroke-width:2px,color:#0D47A1,rx:5,ry:5;
    classDef backbone fill:#FFF3E0,stroke:#E65100,stroke-width:2px,color:#BF360C,rx:5,ry:5;
    classDef pooling fill:#F3E5F5,stroke:#7B1FA2,stroke-width:2px,color:#4A148C,rx:5,ry:5;
    classDef fusion fill:#FCE4EC,stroke:#C2185B,stroke-width:2px,color:#880E4F,rx:5,ry:5;
    classDef dense fill:#FFFDE7,stroke:#FBC02D,stroke-width:2px,color:#F57F17,rx:5,ry:5;
    classDef scaling fill:#E8F5E9,stroke:#2E7D32,stroke-width:2px,color:#1B5E20,rx:5,ry:5;
    classDef output fill:#E0F2F1,stroke:#00695C,stroke-width:3px,color:#004D40,rx:10,ry:10;

    subgraph Inputs [è¾“å…¥å±‚ Inputs]
        direction LR
        IMG_IN[å›¾åƒè¾“å…¥<br>Image Input<br>(B, 3, 512, 512)]:::input
        SEX_IN[æ€§åˆ«è¾“å…¥<br>Sex Input<br>(B, 1)]:::input
    end

    subgraph ImagePath [å›¾åƒç‰¹å¾æå–è·¯å¾„ Image Branch]
        IMG_IN --> BACKBONE[<b>ConvNeXt-Tiny Backbone</b><br>(é¢„è®­ç»ƒç‰¹å¾æå–å™¨)]:::backbone
        BACKBONE -- "(B, 768, H', W')" --> GEM[<b>GeM Pooling</b><br>(å¹¿ä¹‰å¹³å‡æ± åŒ–)]:::pooling
        GEM -- "(B, 768)" --> IMG_VEC(å›¾åƒç‰¹å¾å‘é‡<br>Image Feature Vector):::dense
    end

    subgraph SexPath [æ€§åˆ«ç¼–ç è·¯å¾„ Sex Branch]
        SEX_IN --> SEX_ENC_1[Linear (1->16)<br>BN + ReLU]:::dense
        SEX_ENC_1 --> SEX_ENC_2[Linear (16->32)<br>BN + ReLU]:::dense
        SEX_ENC_2 -- "(B, 32)" --> SEX_VEC(æ€§åˆ«ç‰¹å¾å‘é‡<br>Sex Feature Vector):::dense
    end

    subgraph FusionHead [èåˆä¸å›å½’å¤´ Fusion & Regression Head]
        IMG_VEC --> CONCAT{ç‰¹å¾æ‹¼æ¥<br>Concatenation}:::fusion
        SEX_VEC --> CONCAT
        CONCAT -- "(B, 800)" --> HEAD_1[BN(800) + Dropout(0.5)]:::dense
        HEAD_1 --> HEAD_2[Linear (800->512)<br><b>Mish Activation</b>]:::dense
        HEAD_2 --> HEAD_3[BN(512) + Dropout(0.4)]:::dense
        HEAD_3 --> HEAD_4[Linear (512->64)<br><b>Mish Activation</b>]:::dense
        HEAD_4 --> HEAD_5[Linear (64->1)<br>Raw Output]:::dense
    end

    subgraph OutputLayer [è¾“å‡ºå±‚ Output Layer]
        HEAD_5 --> SIGMOID[<b>Range Scaling</b><br>Sigmoid * MaxAge(240)]:::scaling
        SIGMOID --> FINAL_OUT((æœ€ç»ˆé¢„æµ‹éª¨é¾„<br>Final Bone Age<br>æœˆä»½ Months)):::output
    end

    %% æ ·å¼è°ƒæ•´
    linkStyle default stroke:#455A64,stroke-width:2px,fill:none;
# BoneXage-assessment
Predicting children's bone age using hand X-ray images
# ğŸ¦´ Bone Age Assessment using ConvNeXt & GeM Pooling

[![Python](https://img.shields.io/badge/Python-3.8%2B-blue)](https://www.python.org/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0%2B-orange)](https://pytorch.org/)
[![License](https://img.shields.io/badge/license-MIT-green)](LICENSE)

A PyTorch implementation for precise pediatric **Bone Age Assessment (BAA)** based on hand radiographs.

This project utilizes a modern CNN architecture (**ConvNeXt**) combined with **Generalized Mean (GeM) Pooling** and specific optimizations for medical regression tasks. It achieves superior convergence and stability on the RSNA Pediatric Bone Age dataset compared to traditional baselines.

## ğŸš€ Key Features

* **SOTA Backbone**: Utilizes `ConvNeXt-Tiny` as the feature extractor, offering stronger feature representation than ResNet50 or EfficientNet.
* **GeM Pooling**: Implements learnable Generalized Mean Pooling instead of standard Average Pooling to better capture high-response features (e.g., ossification centers).
* **Multi-Modal Fusion**: Integrates image features with gender encoding to improve prediction accuracy significantly.
* **Numerical Stability**: Applies `Sigmoid * Max_Age` scaling at the output layer. This constraint prevents negative predictions and gradient explosion, ensuring stable training dynamics.
* **AMP Support**: Implements Automatic Mixed Precision (AMP) for faster training and lower GPU memory usage.

## ğŸ› ï¸ Requirements

Ensure you have Python installed. You can install the dependencies via pip:

```bash
pip install torch torchvision numpy pandas matplotlib scikit-learn tqdm pillow


##ğŸ“‚ Dataset Preparation
This project uses the RSNA Pediatric Bone Age Challenge dataset. Please organize your data directory as follows:
data/
â”œâ”€â”€ boneage-training-dataset/   # Training images
â”œâ”€â”€ boneage-test-dataset/       # Test images
â”œâ”€â”€ boneage-training-dataset.csv
â””â”€â”€ boneage-test-dataset.csv


## ğŸ–¥ï¸ Usage
1. Training

Run train.py to start training. The script automatically detects available GPUs and utilizes DataParallel if applicable.
python train.py

2. Inference

Use predict.py to predict the bone age for a single image.
python predict.py <image_list_filename> <sex(M/F)> <image_size> <model_path>

Output:
data/test_imgs/1234.png: 10 years 6 months (126.5 months)

## ğŸ§  Model Architecture
graph LR
    A[Input Image 512x512] --> B[ConvNeXt Backbone]
    B --> C[Feature Maps]
    C --> D[GeM Pooling]
    D --> E[Image Features]
    
    F[Input Sex] --> G[Sex Encoder]
    G --> H[Sex Features]
    
    E --> I[Concat]
    H --> I
    I --> J[FC Layers + Mish]
    J --> K[Sigmoid * 240]
    K --> L[Predicted Age (Months)]

## ğŸ“Š Results
Metric,Value (Approx)
MAE (Mean Absolute Error),To be updated
Backbone,ConvNeXt-Tiny
Input Size,512x512

##ğŸ“ Acknowledgments
    Dataset provided by the RSNA Pediatric Bone Age Challenge.
    ConvNeXt architecture based on A ConvNet for the 2020s.


